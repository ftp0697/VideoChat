import os
import shutil
import subprocess
import sys
import warnings

import gradio as gr
import modelscope_studio as mgr
import uvicorn
from fastapi import FastAPI

warnings.filterwarnings("ignore")

# è®¾ç½®ç¯å¢ƒå˜é‡
os.environ["DASHSCOPE_API_KEY"] = ""
os.environ["is_half"] = "True"  # ä½¿ç”¨åŠç²¾åº¦

# å®‰è£…musetalkä¾èµ–
os.system("mim install mmengine")
os.system('mim install "mmcv==2.1.0"')
os.system('mim install "mmdet==3.2.0"')
os.system('mim install "mmpose==1.2.0"')  # é€‚ç”¨äº torch 2.1.2
# os.system('mim install "mmpose==1.3.2"') # é€‚ç”¨äº torch 2.3.0
shutil.rmtree("./workspaces/results", ignore_errors=True)  # åˆ é™¤æ—§çš„ç»“æœç›®å½•

from src.pipeline import chat_pipeline


def create_gradio():
    """åˆ›å»ºå¹¶è¿”å›Gradioç•Œé¢ã€‚"""
    with gr.Blocks() as demo:
        gr.Markdown(
            """
            <div style="text-align: center; font-size: 32px; font-weight: bold; margin-bottom: 20px;">
            ä¸æ•°å­—äººäº¤è°ˆ
            </div>  
            """
        )
        with gr.Row():
            with gr.Column(scale=2):
                # èŠå¤©æœºå™¨äººç•Œé¢
                user_chatbot = mgr.Chatbot(
                    label="èŠå¤©è®°å½• ğŸ’¬",
                    value=[
                        [
                            None,
                            {
                                "text": "æ‚¨å¥½ï¼Œè¯·é—®æœ‰ä»€ä¹ˆå¯ä»¥å¸®åˆ°æ‚¨ï¼Ÿæ‚¨å¯ä»¥åœ¨ä¸‹æ–¹çš„è¾“å…¥æ¡†ç‚¹å‡»éº¦å…‹é£å½•åˆ¶éŸ³é¢‘æˆ–ç›´æ¥è¾“å…¥æ–‡æœ¬ä¸æˆ‘èŠå¤©ã€‚"
                            },
                        ],
                    ],
                    avatar_images=[
                        {"avatar": os.path.abspath("data/icon/user.png")},
                        {"avatar": os.path.abspath("data/icon/qwen.png")},
                    ],
                    height=500,
                )

                with gr.Row():
                    # UIç»„ä»¶å®šä¹‰
                    avatar_name = gr.Dropdown(
                        label="æ•°å­—äººå½¢è±¡",
                        choices=[
                            "Avatar1 (é€šä¹‰ä¸‡ç›¸)",
                            "Avatar2 (é€šä¹‰ä¸‡ç›¸)",
                            "Avatar3 (MuseV)",
                        ],
                        value="Avatar1 (é€šä¹‰ä¸‡ç›¸)",
                    )
                    chat_mode = gr.Dropdown(
                        label="å¯¹è¯æ¨¡å¼",
                        choices=[
                            "å•è½®å¯¹è¯ (ä¸€æ¬¡æ€§å›ç­”é—®é¢˜)",
                            "äº’åŠ¨å¯¹è¯ (åˆ†å¤šæ¬¡å›ç­”é—®é¢˜)",
                        ],
                        value="å•è½®å¯¹è¯ (ä¸€æ¬¡æ€§å›ç­”é—®é¢˜)",
                    )
                    chunk_size = gr.Slider(
                        label="æ¯æ¬¡å¤„ç†çš„å¥å­æœ€çŸ­é•¿åº¦",
                        minimum=0,
                        maximum=30,
                        value=5,
                        step=1,
                    )
                    tts_module = gr.Dropdown(
                        label="TTSé€‰å‹",
                        choices=["GPT-SoVits", "CosyVoice"],
                        value="CosyVoice",
                    )
                    avatar_voice = gr.Dropdown(
                        label="TTSéŸ³è‰²",
                        choices=[
                            "longxiaochun (CosyVoice)",
                            "longwan (CosyVoice)",
                            "longcheng (CosyVoice)",
                            "longhua (CosyVoice)",
                            "å°‘å¥³ (GPT-SoVits)",
                            "å¥³æ€§ (GPT-SoVits)",
                            "é’å¹´ (GPT-SoVits)",
                            "ç”·æ€§ (GPT-SoVits)",
                        ],
                        value="longwan (CosyVoice)",
                    )

                # å¤šæ¨¡æ€è¾“å…¥ï¼ˆéº¦å…‹é£ï¼‰
                user_input = mgr.MultimodalInput(sources=["microphone"])

            with gr.Column(scale=1):
                # è§†é¢‘æµè¾“å‡º
                video_stream = gr.Video(
                    label="è§†é¢‘æµ ğŸ¬ (åŸºäºGradio 5æµ‹è¯•ç‰ˆï¼Œç½‘é€Ÿä¸ä½³å¯èƒ½å¡é¡¿)",
                    streaming=True,
                    height=500,
                    scale=1,
                )
                # éŸ³è‰²å…‹éš†è¾“å…¥
                user_input_audio = gr.Audio(
                    label="éŸ³è‰²å…‹éš†(å¯é€‰é¡¹ï¼Œè¾“å…¥éŸ³é¢‘æ§åˆ¶åœ¨3-10sã€‚å¦‚æœä¸éœ€è¦éŸ³è‰²å…‹éš†ï¼Œè¯·æ¸…ç©ºã€‚)",
                    sources=["microphone", "upload"],
                    type="filepath",
                )
                # åœæ­¢æŒ‰é’®
                stop_button = gr.Button(value="åœæ­¢ç”Ÿæˆ")

        # ä½¿ç”¨Stateå­˜å‚¨ç”¨æˆ·èŠå¤©è®°å½•å’Œå¤„ç†æ ‡å¿—
        user_messages = gr.State([{"role": "system", "content": None}])
        user_processing_flag = gr.State(False)
        lifecycle = mgr.Lifecycle()

        # äº‹ä»¶å¤„ç†
        # éŸ³è‰²å…‹éš†
        user_input_audio.stop_recording(
            chat_pipeline.load_voice,
            inputs=[avatar_voice, tts_module, user_input_audio],
            outputs=[user_input],
        )
        # åŠ è½½TTSéŸ³è‰²
        avatar_voice.change(
            chat_pipeline.load_voice,
            inputs=[avatar_voice, tts_module, user_input_audio],
            outputs=[user_input],
        )
        # é¡µé¢åŠ è½½æ—¶æ‰§è¡Œ
        lifecycle.mount(
            chat_pipeline.load_voice,
            inputs=[avatar_voice, tts_module, user_input_audio],
            outputs=[user_input],
        )

        # æäº¤ç”¨æˆ·è¾“å…¥
        user_input.submit(
            chat_pipeline.run_pipeline,
            inputs=[
                user_input,
                user_messages,
                chunk_size,
                avatar_name,
                tts_module,
                chat_mode,
                user_input_audio,
            ],
            outputs=[user_messages],
        )
        user_input.submit(
            chat_pipeline.yield_results,
            inputs=[user_input, user_chatbot, user_processing_flag],
            outputs=[user_input, user_chatbot, video_stream, user_processing_flag],
        )

        # é¡µé¢å¸è½½æ—¶åœæ­¢
        lifecycle.unmount(
            chat_pipeline.stop_pipeline,
            inputs=user_processing_flag,
            outputs=user_processing_flag,
        )

        # ç‚¹å‡»åœæ­¢æŒ‰é’®
        stop_button.click(
            chat_pipeline.stop_pipeline,
            inputs=user_processing_flag,
            outputs=user_processing_flag,
        )

    return demo.queue()


if __name__ == "__main__":
    # ä¸»ç¨‹åºå…¥å£
    app = FastAPI()
    gradio_app = create_gradio()
    # å°†Gradioåº”ç”¨æŒ‚è½½åˆ°FastAPI
    app = gr.mount_gradio_app(app, gradio_app, path="/")
    # å¯åŠ¨UvicornæœåŠ¡
    uvicorn.run(app, port=8860, log_level="warning")
